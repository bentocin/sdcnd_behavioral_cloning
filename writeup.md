# **Behavioral Cloning** 


---

**Behavioral Cloning Project**

The goals / steps of this project are the following:
* Use the simulator to collect data of good driving behavior
* Build, a convolution neural network in Keras that predicts steering angles from images
* Train and validate the model with a training and validation set
* Test that the model successfully drives around track one without leaving the road
* Summarize the results with a written report


[//]: # (Image References)

[image1]: ./output_images/center.jpg "Center Driving"
[image2]: ./output_images/recovery1.jpg "Recovery Image"
[image3]: ./output_images/recovery2.jpg "Recovery Image"
[image4]: ./output_images/recovery3.jpg "Recovery Image"
[image5]: ./result.gif "Result"

## Rubric Points
### Here I will consider the [rubric points](https://review.udacity.com/#!/rubrics/432/view) individually and describe how I addressed each point in my implementation.  

---
### Files Submitted & Code Quality

#### 1. Submission includes all required files and can be used to run the simulator in autonomous mode

My project includes the following files:
* clone.py containing the script to create and train the model as well as to perform some preprocessing and data loading
* drive.py for driving the car in autonomous mode
* model.h5 containing a trained convolution neural network 
* writeup_report.md summarizing the results

#### 2. Submission includes functional code
Using the Udacity provided simulator and my adatpted drive.py file, the car can be driven autonomously around the track by executing 
```sh
python drive.py model.h5
```
The images generated by the simulator will be streamed to the python server where they are preprocessed and fed into the model to predict the steering angle. The resulting steering angle is than sent back to the simulator to steer the car.

#### 3. Submission code is usable and readable

The model.py file contains the code for training and saving the convolution neural network. The file shows the pipeline I used for training and validating the model. There are helper functions that read in images using OpenCV and convert them to RGB. Then those images are passed throu the `preprocess_img()` function which crops the image so that the sky and the car are taken out of the image. Right after that the image is resized to 66x200 pixels which is the required format for the used Nvidia model. The final preprocessing step is to convert the image to the YUV color space which is also required by the Nvidia model. Besides there is a `data_generator()` function that reads in the data in batches in order to run memory efficiently. There is also a function to flip (`flip_img()`) the image which is a way of augmenting data and helps the model to generalize and enlarges the amount of data for training. 

### Model Architecture and Training Strategy

#### 1. An appropriate model architecture has been employed

I used the [Nvidia](https://images.nvidia.com/content/tegra/automotive/images/2016/solutions/pdf/end-to-end-dl-using-px.pdf) model which was mentioned in the course and is well known in the area of self-driving. I slightly adapted my model by leaving out the first fully connected layer to avoid overfitting and also add a 50% dropout after the convolutional layers. Additionally, I applied normalization before the image is fed into the network. For activation functions I have tested ReLUs and ELUs and stuck with ELUs in the end.

#### 2. Attempts to reduce overfitting in the model

As mentioned above, I added 50% dropout right after the convolutional layers to avoid overfitting. I also cut out one of the dense layers.
The model was trained and validated on different data sets to ensure that the model was not overfitting. The amount of training data was increased by randomly flipping images horizontally and by using the left and right images as well. For validation only the center images were considered. After training the model was tested by running it through the simulator and ensuring that the vehicle could stay on the track.

#### 3. Model parameter tuning

The model used an Adam optimizer with a learning rate lower than the default learning rate. After some experimentation I stuck with a learning rate of 1e-4. I also varied the batch size between 128 and 256 and tested different sizes of training data which in the end was set to 25600 images per epoch.

#### 4. Appropriate training data

Training data was chosen to keep the vehicle driving on the road. I used a combination of center lane driving, recovering from the left and right sides of the road, and driving the track in reverse direction. The result shows that the recovering datasets help the model to get back to the center but because of the high steering angles can lead to a curvy driving behavior. 


### Model Architecture and Training Strategy

#### 1. Solution Design Approach

I started with a single perceptron to verify that everything is working. Then I used the LeNet architecture to get first results which were of course better than initially but not good enough. At this point I decided to quit experimenting with improvements in LeNet and start experimenting with the Nvidia model. This is also a CNN model which is good for workin with image data in general and the Nvidia model specifically was known to be working well in the area of self-driving.

First results showed that my model was overfitting. I then added 50% dropout after the convolutional layers and removed the first dense layer from the original Nvidia implementation. This showed the expected results and I stuck with that decision.

As a next step I tried to increase the number of epochs which did not improve the result. At this point I decided to lower the learning rate and see whether this improves the outcome. It also had not the expected result but at this stage I was still only using the provided data and I decided to add some recorded datasets. This lead to an improvement of the result and showed that the model was able to still learn.

I decided to increase the amount of training data even further by increasing the number of samples used per epoch. Before I limited it to the number of available center images although I already used left and right images as well as flipping in the data generator. This improved the result further.

During the tuning process I ran sveral simulations and had a look at where the model failed. I then collected more data of the bridge and the tight curves in both directions. I also added some datasets showing recovering maneuvers only.

At the end of the process, the vehicle is able to drive autonomously around the track without leaving the road. However, at some parts it is going from shoulder to shoulder which might be the case because I overrepresented recovering maneuvers in the dataset or because my driving skills with the keyboard were limited.

#### 2. Final Model Architecture

The final model architecture (clone.py lines 84-96) consisted of a convolution neural network with the following layers and layer sizes (activation used is ELU):

* Lambda with 66x200x3 input shape performing normalization
* Convolution 5x5 with 24 filters and 2x2 stride
* Convolution 5x5 with 36 filters and 2x2 stride
* Convolution 5x5 with 48 filters and 2x2 stride
* Convolution 5x5 with 64 filters and 2x2 stride
* Convolution 5x5 with 64 filters and 2x2 stride
* Dropout 50%
* Dense 100
* Dense 50
* Dense 10
* Dense 1

#### 3. Creation of the Training Set & Training Process

To capture good driving behavior, I first recorded several laps on track one using center lane driving in both directions. Here is an example image of center lane driving:

![alt text][image1]

I then recorded the vehicle recovering from the left side and right sides of the road back to center so that the vehicle would learn to correct its position if it gets too close to the shoulders. These images show what a recovery looks like:

![alt text][image2]
![alt text][image3]
![alt text][image4]

To augment the data set, I also flipped images and angles. I also used the left and right images and corrected the steering angle for those images with +/-0.2.

In the end, I had ~56,000 images. These images are read in with a data generator where they are shuffled, preprocessed, and batched to be fed into the network for training. During training some of the images will randomly be flipped to even increase the variety of images. The preprocessing performed in the data generator is done by the function `preprocess_img()` and includes cropping, resizing to 66x200x3, and color space transformation to YUV.

I used this training data for training the model. The validation set helped determine if the model was over or under fitting. I set up callback functions to only save the best model and to stop early when the training is not imrpoving the training any further.

![alt text][image5]