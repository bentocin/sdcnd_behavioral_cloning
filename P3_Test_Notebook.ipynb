{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, Lambda\n",
    "from keras.models import Sequential\n",
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(data_folder, correction):\n",
    "\n",
    "    data = pd.read_csv(os.path.join(data_folder, \"driving_log.csv\"))\n",
    "\n",
    "    # Get the file paths from csv and extract the filename only\n",
    "    X = data[['center', 'left', 'right']].values\n",
    "    # Get the steering values and adjust them for the left and right images\n",
    "    y = data['steering'].values\n",
    "\n",
    "    # Get indices of training and validation data\n",
    "    train_ind, valid_ind = train_test_split(range(X.shape[0]), test_size=0.2, random_state=42)\n",
    "\n",
    "    # For training use center, left, and right images\n",
    "    X_train = np.array([fname.replace(\"\\\\\", \"/\").split(\"/\")[-1] for fname in X[train_ind].flatten(order=\"F\")])\n",
    "    y_train = np.hstack((y[train_ind], y[train_ind]+correction, y[train_ind]-correction))\n",
    "    # For validation only use center images\n",
    "    X_valid = np.array([fname.replace(\"\\\\\", \"/\").split(\"/\")[-1] for fname in X[valid_ind][:,0].flatten(order=\"F\")])\n",
    "    y_valid = y[valid_ind]\n",
    "\n",
    "    return X_train, X_valid, y_train, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_img(file_name):\n",
    "    # Read in image as BGR format\n",
    "    img = cv2.imread(file_name)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flip_img(img, steering):\n",
    "    # Flip image horizontally and adjust steering angle\n",
    "    flip = cv2.flip(img, 1)\n",
    "    angle = -steering\n",
    "    return flip, angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_generator(img_folder, X_data, y_data, batch_size=128, is_training=True):\n",
    "    num_samples = len(X_data)\n",
    "    while 1:\n",
    "        X_data, y_data = shuffle(X_data, y_data)\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            X_batch = X_data[offset:offset+batch_size]\n",
    "            y_batch = y_data[offset:offset+batch_size]\n",
    "\n",
    "            images = []\n",
    "            angles = []\n",
    "\n",
    "            for fname, angle in zip(X_batch, y_batch):\n",
    "                img = read_img(os.path.join(img_folder, fname))\n",
    "                ang = angle\n",
    "                if is_training and np.random.rand() < 0.5:\n",
    "                    img, ang = flip_img(img, angle)\n",
    "                img = preprocess_img(img)\n",
    "                images.append(img)\n",
    "                angles.append(ang)\n",
    "\n",
    "            X = np.array(images)\n",
    "            y = np.array(angles)\n",
    "\n",
    "            yield shuffle(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_img(img):\n",
    "    # Remove the sky and the car\n",
    "    crop = img[65:-25, :, :] \n",
    "    print(type(crop))\n",
    "    # Resize to fit for Nvidia model and change color space accordingly\n",
    "    resize = cv2.resize(crop, (200, 66))\n",
    "    color = cv2.cvtColor(resize, cv2.COLOR_BGR2YUV)\n",
    "    return color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    # Build the Nvidia model with small adaptions\n",
    "    model = Sequential()\n",
    "    model.add(Lambda(lambda x: x/255.0-0.5, input_shape=(66,200,3)))\n",
    "    model.add(Conv2D(24, 5, 5, activation='relu', subsample=(2, 2)))\n",
    "    model.add(Conv2D(36, 5, 5, activation='relu', subsample=(2, 2)))\n",
    "    model.add(Conv2D(48, 5, 5, activation='relu', subsample=(2, 2)))\n",
    "    model.add(Conv2D(64, 3, 3, activation='relu'))\n",
    "    model.add(Conv2D(64, 3, 3, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, X_train, X_valid, y_train, y_valid, batch_size, epochs, img_folder):\n",
    "    # Create callbacks for saving checkpoints and early stopping\n",
    "    checkpoint = ModelCheckpoint(\"model_test-{epoch:02d}.h5\", save_best_only=True)\n",
    "    early_stopping = EarlyStopping(min_delta=0.1, patience=3)\n",
    "\n",
    "    model.compile(loss='mse', optimizer=\"adam\")\n",
    "    # Number of batches to yield from generator for one epoch\n",
    "    train_steps = (X_train.shape[0]//3//batch_size)*batch_size\n",
    "    valid_steps = (X_valid.shape[0]//batch_size)*batch_size\n",
    "    print(train_steps, valid_steps)\n",
    "    # Train the model\n",
    "    model.fit_generator(data_generator(img_folder, X_train, y_train, batch_size, True), train_steps, max_q_size=1, \n",
    "                      nb_epoch=epochs, validation_data=data_generator(img_folder, X_valid, y_valid, batch_size, False),\n",
    "                      nb_val_samples=valid_steps, callbacks=[checkpoint, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_folder = os.path.join(os.path.curdir, \"data\")\n",
    "img_folder = os.path.join(data_folder, \"IMG\")\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "correction = 0.2\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = load_data(data_folder, correction)\n",
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train[:255*3]\n",
    "X_valid = X_valid[:256]\n",
    "y_train = y_train[:255*3]\n",
    "y_valid = y_valid[:256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128 256\n",
      "Epoch 1/3\n",
      "128/128 [==============================] - 4s - loss: 0.0201 - val_loss: 0.0143\n",
      "Epoch 2/3\n",
      "128/128 [==============================] - 2s - loss: 0.0110 - val_loss: 0.0174\n",
      "Epoch 3/3\n",
      "128/128 [==============================] - 2s - loss: 0.0174 - val_loss: 0.0135\n"
     ]
    }
   ],
   "source": [
    "train_model(model, X_train, X_valid, y_train, y_valid, batch_size, epochs, img_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[MoviePy] Building file result.gif with imageio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 125/126 [00:07<00:00, 16.95it/s]\n"
     ]
    }
   ],
   "source": [
    "## To speed up the testing process you may want to try your pipeline on a shorter subclip of the video\n",
    "## To do so add .subclip(start_second,end_second) to the end of the line below\n",
    "## Where start_second and end_second are integer values representing the start and end of the subclip\n",
    "## You may also uncomment the following line for a subclip of the first 5 seconds\n",
    "##clip2 = VideoFileClip('test_videos/solidYellowLeft.mp4').subclip(0,5)\n",
    "clip = VideoFileClip(\"video.mp4\").subclip(16.0,21.0)\n",
    "clip.write_gif(\"result.gif\",fps=25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
